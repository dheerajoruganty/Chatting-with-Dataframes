{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"overview.html","title":"Overview","text":"<pre><code>&lt;p align=\"center\"&gt;\n    &lt;p align=\"center\"&gt;Benchmark any Foundation Model (FM) on any AWS Generative AI service [Amazon SageMaker, Amazon Bedrock, Amazon EKS, Bring your own endpoint etc.]\n    &lt;br&gt;\n&lt;/p&gt;\n</code></pre> Amazon Bedrock | Amazon SageMaker | Amazon EKS | Amazon EC2 <p>A key challenge with FMs is the ability to benchmark their performance in terms of inference latency, throughput and cost so as to determine which model running with what combination of the hardware and serving stack provides the best price-performance combination for a given workload.</p> <p>Stated as business problem, the ask is \u201cWhat is the dollar cost per transaction for a given generative AI workload that serves a given number of users while keeping the response time under a target threshold?\u201d</p> <p>But to really answer this question, we need to answer an engineering question (an optimization problem, actually) corresponding to this business problem: \u201cWhat is the minimum number of instances N, of most cost optimal instance type T, that are needed to serve a workload W while keeping the average transaction latency under L seconds?\u201d</p> <p>W: = {R transactions per-minute, average prompt token length P, average generation token length G}</p> <p>This foundation model benchmarking tool (a.k.a. <code>FMBench</code>) is a tool to answer the above engineering question and thus answer the original business question about how to get the best price performance for a given workload. Here is one of the plots generated by <code>FMBench</code> to help answer the above question (the instance types in the legend have been blurred out on purpose, you can find them in the actual plot generated on running <code>FMBench</code>).</p> <p></p>"},{"location":"overview.html#models-benchmarked","title":"Models benchmarked","text":"<p>Configuration files are available in the configs folder for the following models in this repo.</p>"},{"location":"overview.html#llama3-on-amazon-sagemaker","title":"Llama3 on Amazon SageMaker","text":"<p>Llama3 is now available on SageMaker (read blog post), and you can now benchmark it using <code>FMBench</code>. Here are the config files for benchmarking <code>Llama3-8b-instruct</code> and <code>Llama3-70b-instruct</code> on <code>ml.p4d.24xlarge</code>, <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code> instances.</p> <ul> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> <li>Config file for <code>Llama3-70b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.48xlarge</code>.</li> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> </ul>"},{"location":"report.html","title":"Report","text":"<p>seaborn into plotly markdown template</p>"},{"location":"versions.html","title":"Versions","text":""},{"location":"versions.html#v1046","title":"v1.0.46","text":"<ol> <li>Native model deployment support for EC2 and EKS (i.e. you can now deploy and benchmark models on EC2 and EKS).</li> <li>FMBench is now available in GovCloud.</li> <li>Update to latest version of several packages.</li> </ol>"},{"location":"versions.html#v1045","title":"v1.0.45","text":"<ol> <li>Analytics for results across multiple runs.</li> <li><code>Llama3-70b</code> config files for <code>g5.48xlarge</code> instances.</li> </ol>"},{"location":"versions.html#v1044","title":"v1.0.44","text":"<ol> <li>Endpoint metrics (CPU/GPU utilization, memory utiliztion, model latency) and invocation metrics (including errors) for SageMaker Endpoints.</li> <li><code>Llama3-8b</code> config files for <code>g6</code> instances.</li> </ol>"},{"location":"versions.html#v1042","title":"v1.0.42","text":"<ol> <li>Config file for running <code>Llama3-8b</code> on all instance types except <code>p5</code>.</li> <li>Fix bug with business summary chart.</li> <li>Fix bug with deploying model using a DJL DeepSpeed container in the no S3 dependency mode.</li> </ol>"},{"location":"versions.html#v1040","title":"v1.0.40","text":"<ol> <li>Make it easy to run in the Amazon EC2 without any dependency on Amazon S3 dependency mode.</li> </ol>"},{"location":"versions.html#v1039","title":"v1.0.39","text":"<ol> <li>Add an internal <code>FMBench</code> website.</li> </ol>"},{"location":"versions.html#v1038","title":"v1.0.38","text":"<ol> <li>Support for running <code>FMBench</code> on Amazon EC2 without any dependency on Amazon S3.</li> <li><code>Llama3-8b-Instruct</code> config file for <code>ml.p5.48xlarge</code>.</li> </ol>"},{"location":"versions.html#v1037","title":"v1.0.37","text":"<ol> <li><code>g5</code>/<code>p4d</code>/<code>inf2</code>/<code>trn1</code> specific config files for <code>Llama3-8b-Instruct</code>.<ol> <li><code>p4d</code> config file for both <code>vllm</code> and <code>lmi-dist</code>.</li> </ol> </li> </ol>"},{"location":"versions.html#v1036","title":"v1.0.36","text":"<ol> <li>Fix bug at higher concurrency levels (20 and above).</li> <li>Support for instance count &gt; 1.</li> </ol>"},{"location":"versions.html#v1035","title":"v1.0.35","text":"<ol> <li>Support for Open-Orca dataset and corresponding prompts for Llama3, Llama2 and Mistral.</li> </ol>"},{"location":"versions.html#v1034","title":"v1.0.34","text":"<ol> <li>Don't delete endpoints for the bring your own endpoint case.</li> <li>Fix bug with business summary chart.</li> </ol>"},{"location":"versions.html#v1032","title":"v1.0.32","text":"<ol> <li> <p>Report enhancements: New business summary chart, config file embedded in the report, version numbering and others.</p> </li> <li> <p>Additional config files: Meta Llama3 on Inf2, Mistral instruct with <code>lmi-dist</code> on <code>p4d</code> and <code>p5</code> instances.</p> </li> </ol>"}]}