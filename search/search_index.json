{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index.html#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index.html#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"overview.html","title":"Overview","text":"<pre><code>&lt;p align=\"center\"&gt;\n    &lt;p align=\"center\"&gt;Benchmark any Foundation Model (FM) on any AWS Generative AI service [Amazon SageMaker, Amazon Bedrock, Amazon EKS, Bring your own endpoint etc.]\n    &lt;br&gt;\n&lt;/p&gt;\n</code></pre> Amazon Bedrock | Amazon SageMaker | Amazon EKS | Amazon EC2 <p>A key challenge with FMs is the ability to benchmark their performance in terms of inference latency, throughput and cost so as to determine which model running with what combination of the hardware and serving stack provides the best price-performance combination for a given workload.</p> <p>Stated as business problem, the ask is \u201cWhat is the dollar cost per transaction for a given generative AI workload that serves a given number of users while keeping the response time under a target threshold?\u201d</p> <p>But to really answer this question, we need to answer an engineering question (an optimization problem, actually) corresponding to this business problem: \u201cWhat is the minimum number of instances N, of most cost optimal instance type T, that are needed to serve a workload W while keeping the average transaction latency under L seconds?\u201d</p> <p>W: = {R transactions per-minute, average prompt token length P, average generation token length G}</p> <p>This foundation model benchmarking tool (a.k.a. <code>FMBench</code>) is a tool to answer the above engineering question and thus answer the original business question about how to get the best price performance for a given workload. Here is one of the plots generated by <code>FMBench</code> to help answer the above question (the instance types in the legend have been blurred out on purpose, you can find them in the actual plot generated on running <code>FMBench</code>).</p> <p></p>"},{"location":"overview.html#models-benchmarked","title":"Models benchmarked","text":"<p>Configuration files are available in the configs folder for the following models in this repo.</p>"},{"location":"overview.html#llama3-on-amazon-sagemaker","title":"Llama3 on Amazon SageMaker","text":"<p>Llama3 is now available on SageMaker (read blog post), and you can now benchmark it using <code>FMBench</code>. Here are the config files for benchmarking <code>Llama3-8b-instruct</code> and <code>Llama3-70b-instruct</code> on <code>ml.p4d.24xlarge</code>, <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code> instances.</p> <ul> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> <li>Config file for <code>Llama3-70b-instruct</code> on  <code>ml.p4d.24xlarge</code> and <code>ml.g5.48xlarge</code>.</li> <li>Config file for <code>Llama3-8b-instruct</code> on  <code>ml.inf2.24xlarge</code> and <code>ml.g5.12xlarge</code>.</li> </ul>"}]}